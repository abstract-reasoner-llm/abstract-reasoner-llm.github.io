<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<center>
		<br>
		<span style="font-size:36px "><b>What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models</b></span>
		<br>
		<br>

		<table align=center width=600px>
			<table align=center width=800px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://tttyuntian.github.io/">Tian Yun</a></span>
							<br>
							<span style="font-size:16px">Brown University</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://chensun.me/index.html">Chen Sun</a></span>
							<br>
							<span style="font-size:16px">Brown University</span>
						</center>
					</td>
			
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=sFyrSa8AAAAJ&hl=en">Ellie Pavlick</a></span>
							<br>
							<span style="font-size:16px">Brown University</span>
						</center>
					</td>
				</tr>
			</table>

			<br>

			<table align="center" width="300px">
				<tbody><tr>
					<td align="center" width="120px">
						<center>
							<div class="buttons" style="margin-bottom: 8px;"><a class="btn btn-primary" role="button" href="https://abstract-reasoner-llm.github.io/">[Paper]</a></div>
						</center>
					</td>
					<td align="center" width="120px">
						<center>
							<div class="buttons" style="margin-bottom: 8px;"><a class="btn btn-primary" role="button" href="https://github.com/tttyuntian/abstract_reasoner_llm">[Code]</a></div>
						</center>
					</td>
				</tr>
			</tbody></table>
			
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Recent work has argued that large language models (LLMs) are not "abstract reasoners", citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an "abstract reasoner", and why it matters whether LLMs fit the bill. 
			</td>
		</tr>
	</table>

	<br><hr>

	<!-- <hr>
	<center><h1>Video Presentation</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="TODO_VIDEO_LINK" frameborder="0" allowfullscreen align="center"></iframe>
	</p>
	<hr> -->


	<center><h1>Language Models for Abstract Textual/Visual Reasoning Task (ACRE)</h1></center>
	<table align=center width=100px>
		<center><tr>
			<td align=center width=100px>
				<center>
					<img class="round" style="width:700px" src="./resources/illustration_of_llm_for_reasoning_tasks.png" alt="Illustration of the use of language models for text-based and image-based versions of ACRE. "/>
				</center>
			</td>
		</tr></center>
	</table>
	<br>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Illustration of the use of language models for text-based and image-based versions of ACRE. Each data example will be formulated into a prompt for an LLM to make a <em>prediction</em> for the query. In textual reasoning task, each context frame is represented by a frame caption. In visual reasoning tasks, each context frame is represented by an encoded frame representation.
				</td>
			</tr>
		</center>
	</table>
	<br><hr>


	<center><h1>Overview of Experimental Settings</h1></center>
	<table align=center width=100px>
		<center><tr>
			<td align=center width=100px>
				<center>
					<img class="round" style="width:1200px" src="./resources/illustration.png" alt="Illustration of our experimental settings."/>
				</center>
			</td>
		</tr></center>
	</table>
	<br>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					In Setting (a), we freeze the whole LLM and run evaluations. This is treated as language baseline when image captions are inputs on abstract visual reasoning tasks. In Settings (b) and (c), we freeze the pretrained transformer blocks and finetune only the input layers (i.e., token embedding layer and visual encoder). In Setting (c), we freeze the token embedding layer to study the impact of tuning the visual encoder in a controlled setting. 

					<br><br>Note that the inputs are pure language in Settings (a) and (b), while the inputs are language prompts with image representations in Setting (c).
				</td>
			</tr>
		</center>
	</table>
	<br><hr>


	<center><h1>Results</h1></center>

	<table align=center width=850px>
		<center>
			<tr align=center>
				<td>
					<h3><b>1. Frozen Pretrained LLMs Achieve Low Performance Across A Large Suite of Abstract Reasoning Tasks.</b></h3>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:900px" src="./resources/pretrained_llm_opqa_mcqa.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<br>We first seek to replicate <a href="https://www.ijcai.org/proceedings/2024/0693.pdf">Gendron et al</a>'s finding that frozen pretrained LLMs acheive low performance across a large suite of reasoning tasks. We observe that even though there are small gaps between the original results and the reproduced results, the performance of the pretrained LLMs are still low, indicating that their poor transfer ability to abstract reasoning tasks. What requires additional investigation, however, is whether this poor transfer is interpretable as a lack of abstract reasoning ability.
				</td>
			</tr>
		</center>
	</table>
	
	<br>

	<table align=center width=850px>
		<center>
			<tr align=center>
				<td>
					<h3><b>2. On Abstract Reasoning Tasks, LLaMA2 With A Finetuned Token Embedding Layer Performs Significantly Better Than Its Pretrained Counterpart, AND Can Perform On Par With the LoRA-finetuned LLaMA2.</b></h3>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:900px" src="./resources/finetuned_llm.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<br>Given that pretrained LLMs perform poorly off-the-shelf, it is natural to ask whether they can be adapted to these task, and if so, just how much adaptation is necessary. We explore two ways to finetune the LLMs: (1) finetuning all layers with low-rank adaptation (LoRA); (2) finetunning only the embedding layer of the LLMs. LoRA finetuning has become a standard way of adapting a model to a task and represents an upper bound on how well the model could be made to perform the task under the most permissive conditions. 
					In contrast, finetuning just the embedding layer represents a conceptually different type of transfer with respect to the question of this paper. Namely, finetuning just the embeddings is analogous to changing just the input to the system--e.g., ensuring the input is in the format the system expects--but leaving the system itself unchanged.
					<br><br>
					 We observe that LoRA-finetuned models perform significantly better than their pretrained counterparts, and can even solve ACRE<sup>T</sup> and RAVEN<sup>T</sup> perfectly. Moreover, LLaMA2 with a finetuned embedding layer can perform on par with the LoRA-finetuned LLaMA2.
				</td>
			</tr>
		</center>
	</table>

	<br>

	<table align=center width=850px>
		<center>
			<tr align=center>
				<td>
					<h3><b>3. The Pattern Holds in Abstract Visual Reasoning Task: LLaMA2 with Train-from-scratch Visual Encoder Can Perform Significantly Better Than Their Language-only Counterpart.</b></h3>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=800px>
				<center>
					<td><img class="round" style="width:900px" src="./resources/abstract_visual_reasoning_main.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<br>Given the surprising effectiveness of finetuning just the embedding layer of LLaMA2 on text-only abstract reasoning tasks, we hypothesize that the frozen transformer blocks of a pretrained LLM will perform well on abstract visual reasoning tasks if the visual encoder is tuned for the task. That is, we follow the multimodal LLM framework (MLLM) which consists of a visual backbone, a language backbone, and a linear projection layer which maps visual representations to language latent space. We keep the transformer blocks and the token embedding layer of language backbone frozen, and only train the visual encoder and the projection layer. 
					If this MLLM with a trained visual encoder can perform better than its language backbone with oracle visual perception, then it provides further evidence for the above interpretation of the frozen LLM as a highly transferable system.
					<br><br>
					On ACRE, we observe that LLaMA2 with train-from-scratch visual encoders can perform significantly better than their language-only counterparts, and can even outperform majority of the multimodal state-of-the-art, including IV-CL and LRR, which are pretrained with video data. On MEWL, we observe the same pattern that LLaMA2 with learned visual encoders can outperform prior state-of-the-art and also the language baselines which assume perfect visual perception. 
				</td>
			</tr>
		</center>
	</table>
	
	<br><hr>


	<center><h1>Discussion: What is an Abstract Reasoner?</h1></center>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					The question of whether LLMs are <em>abstract reasoners</em> has consequences for how we understand and thus how we develop increasingly advanced artificial intelligence. The challenge is that there is no consensus for what it means to be an <em>abstract reasoner</em>. In their recent work, <a href="https://www.ijcai.org/proceedings/2024/0693.pdf">Gendron et al</a> operationalize abstract reasoning as the ability to transfer zero-shot to a range of complex reasoning tasks. They find that LLMs perform poorly on this evaluation, and thus conclude that they are not abstract reasoners.
					<br><br>
					In this work, we reproduce <a href="https://www.ijcai.org/proceedings/2024/0693.pdf">Gendron et al</a>'s findings, but push back against their interpretation. In particular, we provide new experiments which show that tuning just the embedding layer is remarkably effective. Indeed, across a variety of textual and multimodal tasks, frozen pretrained LLMs can achieve high levels of performance as long as the input representations are adapted sufficiently for each task.
					<br><br>
					It seems too stringent a criteria to require that that abstract reasoners perform arbitrary tasks on arbitrary inputs without adaptation. By way of counterargument, consider the good old fashioned AI (GOFAI) systems of the 1990s, which typically included symbolic systems internally, e.g., databases implemented in SQL or rules for logical inference implemented in PROLOG. By most intuitive definitions, these databases and rules would be considered "abstract" and the tasks the systems performed over them would be "reasoning". But we would not expect these systems to operate well over a database implemented in MongoDB, or to apply rules defined by Python. Rather, the need to operate on representations of a particular format is a consequence of, not an exception to, the system's abstraction. 
					<br><br>
					Of course, we don't claim that the internal processing of an LLM is exactly analogous to that of a GOFAI system. Of course, in an LLM, tuning the input embedding layer might do more than simply "rerepresent", but rather might encode some task-specific processing as well. But interpreted loosely, the analogy is useful for highlighting how the question of adaptability and transferability relates to the question of abstraction and reasoning.
					<br><br>
					Indeed, this relationship has been considered in depth by philosophers of AI, long before LLMs. For example, <a href="https://direct.mit.edu/books/edited-volume/4889/chapter/623018/True-Believers-The-Intentional-Strategy-and-Why-It">Dennett</a> appeals to transferability in his attempt to describe the difference between human cognition and simpler computational systems:
					<blockquote>
					    Consider the lowly thermostat...we might agree to grant it the capacity for about half a dozen different beliefs...it can believe the room is too cold or too hot, that the boiler is on or off...and so forth...suppose we <em>de-interpret</em> its beliefs and desires, it can believe the A is too F or G...and so forth....by attaching the thermostatic control mechanism to different input and output devices, it could be made to regulate the amount of water in a tank, or the speed of a train for instance...But as systems become perceptually richer and behaviorally more versatile, it becomes harder and harder to make substitutions in the actual links of the system to the world without changing the organization of the system itself. 
					   ...There comes to be a two-way constraint of growing specificity between the device and the environment. Fix the device in any one state and it demands a very specific environment in which to operate properly (you can no longer switch it easily from regulating temperature to regulating speed or anything else); but at the same time, if you do not <em>fix</em> the state it is in, but just plunk it down in a changed environment, its sensory attachments will be sensitive and discriminative enough to respond appropriately to the change...
					</blockquote>
					Although Dennett is not discussing the notion of "abstract reasoners" <em>per se</em>, he observes that intelligent systems do not transfer well unless they are allowed to adapt. Indeed, Dennett argues that this is a defining property, one that differentiates human-like intelligence from simpler (albeit perhaps more abstract) systems such as thermostats.
					<br><br>
					Dennett's argument is relevant here not because LLMs are human-like or even human-level in their reasoning abilities (they are far from it!). Rather, Dennett articulates a position that is implicit in contemporary discussions about LLMs and "abstract reasoning". That is, that we care about how well a system adapts to new environments because adapting well to new environments is a hallmark of intelligence. Indeed, this is often cited explicitly as the motivation for studies of this nature (e.g., "the question of whether or not LLMs can perform human-like reasoning remains open..." (Gendron et al. 2024)). But if evaluating human-likeness or human-levelness is the motivation for studying abstract reasoning, then arguments such as Dennett's provide a compelling case against using zero-shot transfer ability as a relevant metric.
					<br><br>
					Of course, there is another, more practical, argument for why we might care about whether LLMs are abstract reasoners, which is simply that we want LLMs to transfer well zero-shot to many tasks in order to facilitate easier, cheaper, and more efficient development of systems. Indeed, the thermostat's highly abstract design is a feature, not a bug. This type of hardware abstraction is what allows similar components and control mechanisms to be readily repurposed to support many types of use cases. A "human like" thermostat might be very undesirable.
					<br><br>
					Thus, before seeking to answer the question of whether LLMs are "abstract reasoners", we must first determine, as a community, why we care. Do we care because we want to understand how human-like they are, or do we care because we want to facilitate more efficient technological progress? Almost certainly, we care about both, but we should not expect the same experiments to bear on both lines of inquiry. Finding clarity around these questions - what is an abstract reasoner and why do we care about building one? - is the essential next step if we are to make progress toward either, or both, goals. 
				</td>
			</tr>
		</center>
	</table>
	<br><hr>


	<table align=center width=650px>
		<center><h1>Paper and BibTex</h1></center>
		<tr>
			<td><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></td>
			<td><span style="font-size:14pt">Tian Yun, Chen Sun, Ellie Pavlick.<br>
				<b>What is an “Abstract Reasoner”? Revisiting Experiments and Arguments about Large Language Models</b><br>
				Under Review.<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br><br>

	<!-- <table align=center width=850px>
		<center>
			<tr>
				<td style="background-color:#e1e1e1">
					<pre><code>
  @inproceedings{hua2024mothello,
	title={mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?},
	author={Tianze Hua and Tian Yun and Ellie Pavlick},
	booktitle={2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	year={2024}
  }
					</code></pre>
				</td>
			</tr>
		</center>
	</table> -->

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					<br><br>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

